# -*- coding: utf-8 -*-
"""mece6616_Spring2023_Project4_ql2465.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PS2S2Dp8kZVx5VedDBhX-5_jVWFMcFIS

# ***Important***

**Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/172081/pages/assignment-instructions) page on Courseworks2 to learn the workflow for completing this project.**

<ins> Different from project 1 and 2</ins>, apart from the link to your notebook, you are also required to submit 2 additional files namely the collected data `data.pkl` and your chosen model checkpoint `dynamic.pth`.

## Project Setup
"""

# DO NOT CHANGE

# After running this cell, the folder 'mecs6616_sp23_project3' will show up in the file explorer on the left (click on the folder icon if it's not open)
# It may take a few seconds to appear
!git clone https://github.com/roamlab/mecs6616_sp23_project4.git

# DO NOT CHANGE

# move all needed files into the working directory. This is simply to make accessing files easier
!mv /content/mecs6616_sp23_project4/* /content/

!pip install ray

"""# Starter Code Explanation

This project uses the same environment as in project 3 which consists of an n linked robot arm. We provide code for teacher dynamics and the controller. We also provide scripts for collecting data, skeleton classes for implementing student dynamics and finally training and evaluating the model.

Similar to last time we define a Robot class inside `robot.py` which provides the interface for controlling the robot arm i.e it provides you with some functions to set/get the state and set the action for the arm and take a step using the `Robot.advance()` method. The state of the arm is a 2n-dimensional vector : n joint positions [rad] + n joint velocities [rad/s] and the action is defined as the n torques (in N-m) applied to n joints respectively.

In `arm_dynamic_base.py` we define the base class for forward dynamics for the arm. The ground truth forward dynamics are defined in `arm_dynamics_teacher.py`. The student dynamics which internally uses a neural network model is defined in `arm_dynamics_student.py`

You are welcome to look in-depth to understand how the ground truth forward dynamics is computed for an arm, given its number of links, link mass, and viscous friction of the environment - however this is not necessary to successfully complete this assignment.


The `models.py` file provides the base class for neural network to learn forward model and also contains skeleton code for you to implement the network architecture for arms with different number of links. We will revisit this again in when providing instructions for training below.

# Part 1: Implement Model Predictive Control

You will implement this controller by completing the MPC class. Specifically, you will implement the compute_action() method by following the algorithm discussed in the lecture. As with previous projects you are free to implement additional methods as needed or change the initialization if need be. While scoring your controller, you will be creating an instance of the MPC class and passing it to the scoring function so ensure that the arguments to the compute_action method remain the same.

Although you do not need to understand how the ArmDynamicsTeacher class works, you could use the compute_fk() method from the class. This will allow you to convert from the state value (represented as array of shape (2*n, 1) where n is num_links) to final end effector position (x, y position of the end effector). Similary we can also compute the velocity of the end effector with the code below:
```
pos_ee = dynamics.compute_fk(state)
vel_ee = dynamics.compute_vel_ee(state)
```
"""

from collections import defaultdict
import numpy as np
class MPC:

  def __init__(self):
      self.control_horizon = 10
      # Define other parameters here
      self.N = 40

      self.decay = 0.98
      self.delta0 = 0.1 #0.115 #0.314
      self.deltamin = 0.01

      self.pos_w = 80
      self.vel_w = 1

  def trajectory(self, dynamics, state, action):

      # print(state)
      trajectory = np.zeros((len(state), self.N))
      trajectory[:, [0]] = state
      for i in range(self.N-1):
          next_state = dynamics.advance(trajectory[:, [i]], action)
          trajectory[:, [i+1]] = next_state
      # print(trajectory)
      return trajectory

  def compute_loss(self, dynamics, trajectory, goal):

      cost_pos = 0
      cost_vel = 0
      for i in range(trajectory.shape[1]):
        state = trajectory[:,i]
        vel_ee = dynamics.compute_vel_ee(state)
        pos_ee = dynamics.compute_fk(state)
        cost_vel += np.linalg.norm(vel_ee) ** 2
        cost_pos += np.linalg.norm(goal - pos_ee) ** 2

      cost = cost_pos * self.pos_w + cost_vel*self.vel_w


      return cost

  def compute_action(self, dynamics, state, goal, action):
      # Put your code here. You must return an array of shape (num_links, 1)

      # Don't forget to comment out the line below
      # raise NotImplementedError("MPC not implemented")
      # best_action = np.ones((arm.dynamics.get_action_dim(), 1))
      #print(goal)
        # np.random.seed(0)
        trajectory = self.trajectory(dynamics, state, action)

        best_cost = float("inf")
        # action = np.zeros((arm.dynamics.get_action_dim(), 1))
        best_action = np.copy(action)
        cur_action = np.tile(best_action, (self.N, 1))
        # print(action)

        for i in reversed(range(len(action))):
            improving = True
            flag = 0

            while improving and flag < 40:
                # improving = True
                flag +=1

                delta = self.delta0 * self.decay ** flag
                delta = np.max([delta, self.deltamin])

                cur_p = np.copy(best_action)
                cur_p[i] += delta
                p_trajectory = self.trajectory(dynamics, state, cur_p)
                p_cost = self.compute_loss(dynamics, p_trajectory, goal)

                cur_m = np.copy(best_action)
                cur_m[i] -= delta
                m_trajectory = self.trajectory(dynamics, state, cur_m)
                m_cost = self.compute_loss(dynamics, m_trajectory, goal)

                min_cost = np.min([best_cost, p_cost, m_cost])

                improving = False
                if min_cost < best_cost:
                    best_cost = min_cost
                    improving = True
                    index = np.argmin([p_cost, m_cost])
                    com = [cur_p, cur_m]
                    best_action = com[index]
        # print(best_cost)
        # print(best_action)
        return best_action

"""# Manually testing the controller
This part is for you to manually check the performance of your controller before you are ready for it be evaluated by our scoring function.
To test your implementation run the following code. Feel free to play around with the cell or change the num_links / goal positions . You can define your controller however you would like to and then switch on gui to see how close your end effectors get to the goal position

Every time step within the environment is 0.01s, which is defined in the dynamics as `dt`.

The MPC class has a `control_horizon` variable which represents the frequency at which `controller.compute_action()` will be called

In the scoring function you will be evaluated on the distance of your end effector to the goal position and the velocity of the end effector.
"""

import sys
import numpy as np
from arm_dynamics_teacher import ArmDynamicsTeacher
from robot import Robot
from render import Renderer
from score import *
import torch
import time
import math
np.set_printoptions(suppress=True)

# Teacher arm with 3 links
dynamics_teacher = ArmDynamicsTeacher(
    num_links=2,
    link_mass=0.1,
    link_length=1,
    joint_viscous_friction=0.1,
    dt=0.01)

arm = Robot(dynamics_teacher)
arm.reset()

# gui = True
gui = False

if gui:
  renderer = Renderer()
  time.sleep(1)

# Controller
controller = MPC()

# Resetting the arm will set its state so that it is in the vertical position,
# and set the action to be zeros
arm.reset()
action = np.zeros((arm.dynamics.get_action_dim(), 1))
# Choose the goal position you would like to see the performance of your controller
goal = np.zeros((2, 1))
goal[0, 0] = 0.02
goal[1, 0] = 0
# pos = np.random.uniform(0.05, 0.2)
# ang = np.random.uniform(math.pi, 2*math.pi)
# goal[0, 0] = pos*math.cos(ang)
# goal[1, 0] = pos*math.sin(ang)
# print(goal)
# goal = [ 0.6814821, -1.61185674]
arm.goal = goal

dt = 0.01
time_limit = 2.5 # 2.5
num_steps = round(time_limit/dt)

# Control loop
for s in range(num_steps):
  # action = np.zeros((arm.dynamics.get_action_dim(), 1))
  t = time.time()
  arm.advance()

  if gui:
    renderer.plot([(arm, "tab:blue")])
  time.sleep(max(0, dt - (time.time() - t)))

  if s % controller.control_horizon==0:
    state = arm.get_state()

    # Measuring distance and velocity of end effector
    pos_ee = dynamics_teacher.compute_fk(state)
    dist = np.linalg.norm(goal-pos_ee)
    vel_ee = np.linalg.norm(arm.dynamics.compute_vel_ee(state))
    print(f'At timestep {s}: Distance to goal: {dist}, Velocity of end effector: {vel_ee}')

    action = controller.compute_action(arm.dynamics, state, goal, action)
    arm.set_action(action)

"""# Grading and Evaluation for Part 1
Your controller will be graded on 6 tests. 2 tests each for the 1-link ,2-link, and 3-link arms. The arm will start off in the initial state with the arms pointing stright down. The testing criteria depend on the distance and the velocity of the end effectors . Each test will run the robot arm for **5.0 seconds**. At the end of the 5 seconds the test will be:

A success if your end effectors meet this criteria:
`distance_to_goal < 0.1 and vel_ee < 0.5`

A partial success if your end effectors meet this criteria:
`distance_to_goal < 0.2 and vel_ee < 0.5`

"""

# Scoring using score_mpc
controller = MPC()
gui = False

# DO NOT CHANGE
score_mpc_true_dynamics(controller, gui)

"""### Part 2.1: Model Architecture
Beyond this point you will be focusing on the 2 link arm only.

We have a base class Model and a subclass for the 2-link arm. The class Model is a base class for our models.  In compute_next_state() method,
you have to use the trick to use joint accelerations to compute the next state similar to what you did in Project 3.

In the `Model2Link` class you will use a neural network to compute the joint accelerations by implementing `compute_qddot()` method. This will take 6 values (2 joint angles, 2 joint velocities and 2 actions applied to the arm) and output 2 joint acceleration values

Do not change the arguments for the `__init__()` method even if you do not use them.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
	def __init__(self, num_links, time_step):
		super().__init__()
		self.num_links = num_links
		self.time_step = time_step

	def forward(self, x):
		qddot = self.compute_qddot(x)
		state = x[:, :2*self.num_links]
		next_state = self.compute_next_state(state, qddot)
		return next_state

	def compute_next_state(self, state, qddot):

		# Your code goes here
		dt = self.time_step
		velo_old = state[:, self.num_links:2*self.num_links]
		velo_new = velo_old + qddot * torch.tensor(dt)
		angle_old = state[:, :self.num_links]
		angle_new = angle_old + torch.tensor(0.5) * (velo_old + velo_new) * torch.tensor(dt)
		next_state = torch.cat([angle_new, velo_new], dim=1)
		return next_state

	def compute_qddot(self, x):
		pass

class Model2Link(Model):
	def __init__(self, time_step):
			super().__init__(2, time_step)
			self.fc1 = nn.Linear(6, 1024)
			# self.fc2 = nn.Linear(1024, 512)
			self.fc3 = nn.Linear(1024, 256)
			self.fc4 = nn.Linear(256, 64)
			self.fc5 = nn.Linear(64, 2)

	def compute_qddot(self, x):
    # Your code goes here
			x = F.relu(self.fc1(x))
			# x = F.relu(self.fc2(x))
			x = F.relu(self.fc3(x))
			x = F.relu(self.fc4(x))
			x = self.fc5(x)

			return x

"""### Part 2.2: Collect Data
Similar to project 3, we will collect data which will be used to learn a forward model for our 2 link robot arm. Once we have learnt a forward model you will be evaluated on your MPC Controller that uses the learnt dynamics model instead of the true dynamics.

You can modify the collect_data function or write any of your own functions however you choose to. You will be evaluated on the **2 Link Robot**
"""

# Teacher arm with 3 links
import ray
dynamics_teacher = ArmDynamicsTeacher(
    num_links=2,
    link_mass=0.1,
    link_length=1,
    joint_viscous_friction=0.1,
    dt=0.01)

arm = Robot(dynamics_teacher)
arm.reset()

controller = MPC()

# ray.shutdown()
# ray.init()

# @ray.remote(num_returns = 2, num_cpus = 2)
def collect_data(arm):

  # ---
  # You code goes here. Replace the X, and Y by your collected data
  # Control the arm to collect a dataset for training the forward dynamics.
  num_steps = 10
  num_data = 800
  num_change = 25
  X = np.zeros((num_steps*num_data*num_change, arm.dynamics.get_state_dim() + arm.dynamics.get_action_dim()))
  Y = np.zeros((num_steps*num_data*num_change, arm.dynamics.get_state_dim()))
  # ---
  for i in range(num_data):
        print(i)
        initial_state = np.zeros((arm.dynamics.get_state_dim(), 1))  # position + velocity
        initial_state[0] = -math.pi / 2.0
        arm.set_state(initial_state)
        action = np.zeros((arm.dynamics.get_action_dim(), 1))

        goal = np.zeros((2,1))
        pos = np.random.uniform(0.05, 1.95)
        ang = np.random.uniform(math.pi, 2*math.pi)
        goal[0, 0] = pos*math.cos(ang)
        goal[1, 0] = pos*math.sin(ang)
        # print(goal)
        # goal = [ 0.6814821, -1.61185674]
        # arm.goal = goal
        for n in range(num_change):
            # action[0] = 1.0*(1.2-2.4*np.random.rand())
            # action[1] = 1.0*(1.2-2.4*np.random.rand())
            # print(action)
            state = arm.get_state()
            action = controller.compute_action(dynamics_teacher, state, goal, action)
            arm.set_action(action)

            for j in range(num_steps):
                # X[i*num_steps+j,0:4] = arm.get_state().flatten()
                # X[i*num_steps+j,4:6] = action.flatten()

                # arm.advance()
                # Y[i*num_steps+j,:4] = arm.get_state().flatten()
                X[i*num_change*num_steps+ n*num_steps +j,0:4] = arm.get_state().flatten()
                X[i*num_change*num_steps+ n*num_steps +j,4:6] = action.flatten()

                arm.advance()
                Y[i*num_change*num_steps+ n*num_steps +j,:4] = arm.get_state().flatten()

  # print(X)
  # print(Y)
  return X, Y

import pickle

# Call the function you have defined above to collect data
X, Y = collect_data(arm)
save_dir = 'dataset'
if not os.path.exists(save_dir):
  os.makedirs(save_dir)

# Save the collected data in the data.pkl file
data = {'X': X, 'Y': Y}
pickle.dump(data, open(os.path.join(save_dir, 'data.pkl'), "wb" ))
print("data  collected")

"""### Part 2.3: Training the forward model
By now you would be familiar with the basic skeleton of training a forward model.

The starter code already creates the dataset class and loads the dataset with a random 0.8/0.2 train/test split for you. This script should save the model that it trains. You should use a specific procedure for saving, outlined below.

In machine learning, it is a very good practice to save not only the final model but also the checkpoints. Our starter code already configures save_dir for you and for each epoch of your training, you should use the following code to save the model at that epoch.

```
model_folder_name = f'epoch_{epoch:04d}_loss_{test_loss:.8f}'
if not os.path.exists(os.path.join(args.save_dir, model_folder_name)):
    os.makedirs(os.path.join(args.save_dir, model_folder_name))
torch.save(model.state_dict(), os.path.join(args.save_dir, model_folder_name, 'dynamics.pth'))
print(f'model saved to {os.path.join(args.save_dir, model_folder_name, "dynamics.pth")}\n')
```
The output from running this code should be a folder as below:

```
models/
    2021-03-24_23-57-50/
        epoch_0001_loss_0.00032930/
            dynamics.pth
        epoch_0002_loss_0.00009413/
            dynamics.pth   
        ...  
```
You can implement the functions below as you please to collect data
"""

import torch
from torch.utils.data import Dataset, DataLoader, random_split
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import tqdm
import pickle
import torch.optim as optim
import argparse
import time

class DynamicDataset(Dataset):
  def __init__(self, datafile):
    data = pickle.load(open(datafile, 'rb'))
    # X: (N, 6), Y: (N, 4)
    self.X = data['X'].astype(np.float32)
    self.Y = data['Y'].astype(np.float32)

  def __len__(self):
    return self.X.shape[0]

  def __getitem__(self, idx):
    return self.X[idx], self.Y[idx]


def train_one_epoch(model, train_loader, total_train):
    model.train()

    learning_rate = 0.0005
    optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)
    criterion = nn.MSELoss()
    # scheduler = ExponentialLR(self.optimizer, gamma=0.99)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.95, patience=5, verbose=True, min_lr=1e-5)

    total_loss = 0.0
    for i, data in enumerate(train_loader):
        features = data[0]
        labels = data[1]
        optimizer.zero_grad()
        predictions = model(features)
        loss = criterion(predictions, labels)
        loss.backward()
        total_loss += loss.item()
        optimizer.step()
        # print(i)
    scheduler.step(total_loss)

def test(model, test_loader, total_test):
    model.eval()
    criterion = nn.MSELoss()
    test_loss = 0
    # ---
    for i, data in enumerate(test_loader):
        feature = data[0]
        label = data[1]
        predictions = model(feature)
        # print(predictions.shape)
        loss= criterion(predictions, label)
        test_loss += loss.item()
        cur_loss = test_loss/(i+1)

    print(f"loss:{cur_loss}")
    return cur_loss

def train_forward_model():

	# --
	# Implement this function
  # --

  # Keep track of the checkpoint with the smallest test loss and save in model_path
  model_path = None
  max_test_loss = 1e4
  model = Model2Link(0.01)

  datafile = 'dataset/data.pkl'
  split = 0.2
  dataset = DynamicDataset(datafile)
  dataset_size = len(dataset)
  test_size = int(np.floor(split * dataset_size))
  train_size = dataset_size - test_size
  train_set, test_set = random_split(dataset, [train_size, test_size])

  train_loader = torch.utils.data.DataLoader(train_set, batch_size=200, shuffle=True)
  test_loader = torch.utils.data.DataLoader(test_set, batch_size=200, shuffle=True)
  total_train = train_set.__len__()
  total_test = test_set.__len__()
  # The name of the directory to save all the checkpoints
  timestr = time.strftime("%Y-%m-%d_%H-%M-%S")
  model_dir = os.path.join('models', timestr)

  epochs=50
  model = Model2Link(0.01)
  loss = float('inf')
  for epoch in range(1, 1 + epochs):
    train_one_epoch(model, train_loader, total_train)

    test_loss = test(model, test_loader, total_test)
    if test_loss < loss:
        loss = test_loss
        best = epoch
        model_folder_name = f'epoch_{epoch:04d}_loss_{test_loss:.8f}'
        if not os.path.exists(os.path.join(model_dir, model_folder_name)):
            os.makedirs(os.path.join(model_dir, model_folder_name))
        torch.save(model.state_dict(), os.path.join(model_dir, model_folder_name, 'dynamics.pth'))
  print(best)
  return os.path.join(model_dir, model_folder_name, 'dynamics.pth')

"""# Run Model"""

model_path = train_forward_model()

model_path = "/content/models/2023-04-15_05-49-39/epoch_0025_loss_0.00000258/dynamics.pth"
# model_path = "/content/dynamics.pth"

"""### Part 2.4: Completing ArmDynamicsStudent

After you are done with training, you need to complete ArmDynamicsStudent class following the comments below to load the saved checkpoint (in function init_model) and then use it to predict the new state given the current state and action (in function dynamics_step). Please do not modify the arguments to those functions, even though you might not use all of them.
"""

from arm_dynamics_base import ArmDynamicsBase

class ArmDynamicsStudent(ArmDynamicsBase):
    def init_model(self, model_path, num_links, time_step, device):
        # ---
        # Your code hoes here
        # Initialize the model loading the saved model from provided model_path
        self.model = Model2Link(time_step)
        # ---
        self.model_loaded = True
        best_check_point = torch.load(model_path)
        # best_check_point = torch.as_tensor(model_path)
        self.model.load_state_dict(best_check_point)

    def dynamics_step(self, state, action, dt):
        if self.model_loaded:
            self.model.eval()
            state = np.array(state).reshape(1, -1)
            action = np.array(action).reshape(1, -1)
            X_predict = np.concatenate((state, action), axis=1)
            # print(X_predict.shape)
            new_state = self.model(torch.from_numpy(X_predict).float()).detach().numpy()

            return new_state.reshape(-1, 1)

            # ---
        else:
            return state

"""### Manually Testing the MPC Controller with the learnt dynamics model
We will now use the learnt dynamics model that you have trained. The model is loaded in the dynamics.init_model method. You can modify the goal positions to see how well is the controller performing similar to what you did before. Feel free to play around with the code in this cell to test your performance before the grading part.

"""

import sys
import numpy as np
from arm_dynamics_teacher import ArmDynamicsTeacher
from robot import Robot
from render import Renderer
from score import *
import torch
import time

# Teacher arm with 3 links
dynamics_teacher = ArmDynamicsTeacher(
    num_links=2,
    link_mass=0.1,
    link_length=1,
    joint_viscous_friction=0.1,
    dt=0.01)

arm = Robot(dynamics_teacher)
arm.reset()

gui = False
action = np.zeros((arm.dynamics.get_action_dim(), 1))
if gui:
  renderer = Renderer()
  time.sleep(1)


# Controller
controller = MPC()
dynamics_student = ArmDynamicsStudent(
    num_links=2,
    link_mass=0.1,
    link_length=1,
    joint_viscous_friction=0.1,
    dt=0.01)
device = torch.device('cpu')

# model_path should have the path to the best model that you have trained so far
dynamics_student.init_model(model_path, 2, 0.01, device)

# Control loop
action = np.zeros((arm.dynamics.get_action_dim(), 1))
# pos = np.random.uniform(0.05, 0.2)
# ang = np.random.uniform(math.pi, 2*math.pi)
# goal[0, 0] = pos*math.cos(ang)
# goal[1, 0] = pos*math.sin(ang)
print(goal)
goal[0, 0] = -0.17808226
goal[1, 0] = -0.05000058
arm.goal = goal

dt = 0.01
time_limit = 2.5
num_steps = round(time_limit/dt)
for s in range(num_steps):
  t = time.time()
  arm.advance()

  if gui:
    renderer.plot([(arm, "tab:blue")])
  time.sleep(max(0, dt - (time.time() - t)))

  if s % controller.control_horizon==0:
    state = arm.get_state()

    pos_ee = dynamics_teacher.compute_fk(state)
    dist = np.linalg.norm(goal-pos_ee)
    vel_ee = np.linalg.norm(arm.dynamics.compute_vel_ee(state))
    print(f'At timestep {s}: Distance to goal: {dist}, Velocity of end effector: {vel_ee}')

    action = controller.compute_action(dynamics_student, state, goal, action)

    arm.set_action(action)

"""## Grading and Evaluation of Part 2
You will be evaluated on how well your controller+learnt dynamics works together. The scoring functions consists of 16 random test goals all of which will be below the x axis and between 0.05 to 1.95 lengths away from the origin.
The controller will call the compute_action method from your MPC class and apply the action for 10 timesteps
```
action = controller.compute_action(dynamics_student, state, goal, action)
```

Each test will run the robot arm for **2.5 seconds**. At the end of the 2.5 seconds the test will be:

A success if your end effectors meet this criteria:
`distance_to_goal < 0.2 and vel_ee < 0.5`

A partial success if your end effectors meet this criteria:
`distance_to_goal < 0.3 and vel_ee < 0.5`

You need 15 out of the 16 tests to succeed to get a full score
"""

controller = MPC()
dynamics_student = ArmDynamicsStudent(
    num_links=2,
    link_mass=0.1,
    link_length=1,
    joint_viscous_friction=0.1,
    dt=0.01)
# model_path = '/content/models/2023-04-15_05-49-39/epoch_0025_loss_0.00000258/dynamics.pth'
gui=False

# DO NOT CHANGE
score_mpc_learnt_dynamics(controller, dynamics_student, model_path, gui)

"""Hints and Suggestions:
1. You can use your MPC Controller in your data collection to gather better training samples
2. A good cost function to evaluate your trajectory in MPC is very important and you can use both distance and velocity metrics to define the cost function.
3. As mentioned in the lecture, a constant torque with pseudo gradients seems to work well for this project. You can also use multiple delta values to gather more trajectories to choose from.
4. Since we are passing the MPC object to the controller you can instantiate the MPC class with different parameters like the planning horizon, delta values etc.
5. To speed up data collection, avoid using np.concatenate(), np.stack() or np.append() like functions on your X and Y arrays. Instead, initialize X and Y arrays with all zeros using the correct shape and then fill in the values one by one. This is much faster in numpy
"""